{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf066cba-870c-4b14-bdc5-83821e5a521d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Libraries (Python)"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import json\n",
    "from azure.cosmos import CosmosClient, exceptions  # Added\n",
    "\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62e0165a-9df7-45a1-86f4-01eab5ae0400",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display Secret"
    }
   },
   "outputs": [],
   "source": [
    "print(dbutils.secrets.listScopes())\n",
    "print(dbutils.secrets.list(\"cosmosdb\"))\n",
    "\n",
    "secret_value = dbutils.secrets.get(scope=\"cosmosdb\", key=\"connection-string\")\n",
    "print(f\"Retrived secret (hiddn): {secret_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81703d66-36df-402f-b092-c718d5f4e0b3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add family to Parquet Files"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Define file paths with their respective names\n",
    "file_details = [\n",
    "    {\"path\": \"dbfs:/dbfs/tmp/parquet_files/bushings.parquet\", \"name\": \"bushings\"},\n",
    "    {\"path\": \"dbfs:/dbfs/tmp/parquet_files/gaskets.parquet\", \"name\": \"gaskets\"},\n",
    "    {\"path\": \"dbfs:/dbfs/tmp/parquet_files/screws___bolts.parquet\", \"name\": \"screws__bolts\"}\n",
    "]\n",
    "\n",
    "# Process each file independently\n",
    "for file_detail in file_details:\n",
    "    file_path = file_detail[\"path\"]\n",
    "    file_name = file_detail[\"name\"]\n",
    "    \n",
    "    # Read the Parquet file\n",
    "    df = spark.read.parquet(file_path)\n",
    "    \n",
    "    # Add the file name as a new column\n",
    "    df_with_file_name = df.withColumn(\"family\", lit(file_name))\n",
    "\n",
    "    # Delete the nul rows\n",
    "    cleaned_df = df_with_file_name.filter(\n",
    "        F.col(\"part_number\").isNotNull() &\n",
    "        F.col(\"category\").isNotNull() &\n",
    "        F.col(\"subcategory\").isNotNull()\n",
    "    )\n",
    "    \n",
    "    # Process the DataFrame (Example: display it or apply transformations)\n",
    "    print(f\"Processing file: {file_name}\")\n",
    "    display(cleaned_df)\n",
    "    cleaned_df.printSchema()\n",
    "    \n",
    "    # For example, save it back to another location or perform analytics\n",
    "    cleaned_df.write.mode('overwrite').parquet(f\"dbfs:/dbfs/tmp/parquet_files/{file_name}{1}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6613eec2-0642-431f-bdf0-cdc713af3257",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configuration (Python)"
    }
   },
   "outputs": [],
   "source": [
    "# Cell: Configuration\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# File paths in DBFS\n",
    "FILE_PATHS = {\n",
    "    \"Bushings\": \"dbfs:/dbfs/tmp/parquet_files/bushings1.parquet\",\n",
    "    \"Gaskets\": \"dbfs:/dbfs/tmp/parquet_files/gaskets1.parquet\",\n",
    "    \"Screws__Bolts\": \"dbfs:/dbfs/tmp/parquet_files/screws__bolts1.parquet\"\n",
    "}\n",
    "\n",
    "# CosmosDB NoSQL API Configuration (NOT MongoDB)\n",
    "try:\n",
    "    COSMOS_ENDPOINT = \"\"\n",
    "    COSMOS_KEY = \"\"\n",
    "except:\n",
    "    # Fallback: Direct credentials (NOT recommended for production)\n",
    "    COSMOS_ENDPOINT = \"\"\n",
    "    COSMOS_KEY = \"\"\n",
    "    print(\"⚠ Using direct credentials - consider using Databricks Secrets\")\n",
    "\n",
    "DB_NAME = \"business\"\n",
    "CONTAINER_NAME = \"Feature_Statistics\"\n",
    "\n",
    "# Configure Spark Cosmos DB Connector\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog\", \"com.azure.cosmos.spark.CosmosCatalog\")\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountEndpoint\", COSMOS_ENDPOINT)\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountKey\", COSMOS_KEY)\n",
    "\n",
    "# Classification columns\n",
    "CLASSIFICATION_COLUMNS = [\"family\"]\n",
    "\n",
    "# Generate batch ID\n",
    "BATCH_ID = f\"BATCH-{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Cosmos Endpoint: {COSMOS_ENDPOINT}\")\n",
    "print(f\"  Database: {DB_NAME}\")\n",
    "print(f\"  Container: {CONTAINER_NAME}\")\n",
    "print(f\"  Batch ID: {BATCH_ID}\")\n",
    "print(f\"  Classification columns: {CLASSIFICATION_COLUMNS}\")\n",
    "print(f\"  Files to process: {len(FILE_PATHS)}\")\n",
    "print(\"✓ Spark Cosmos DB Connector configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b14745f-1d82-41e6-8a6d-fdc81dc52f2f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Helper Functions - Data Type Detection (Python)"
    }
   },
   "outputs": [],
   "source": [
    "def is_numeric_column(dtype):\n",
    "    \"\"\"Check if column is numeric (continuous data)\"\"\"\n",
    "    numeric_types = [\n",
    "        'int', 'long', 'float', 'double', \n",
    "        'decimal', 'bigint', 'smallint', 'tinyint'\n",
    "    ]\n",
    "    return any(t in str(dtype).lower() for t in numeric_types)\n",
    "\n",
    "def is_categorical_column(dtype):\n",
    "    \"\"\"Check if column is categorical (string, boolean)\"\"\"\n",
    "    categorical_types = ['string', 'boolean']\n",
    "    return any(t in str(dtype).lower() for t in categorical_types)\n",
    "\n",
    "print(\"✓ Data type detection functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94909416-2be6-4aa3-a9fe-7790347425dd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Helper Functions - Continuous Statistics (Python)"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_continuous_statistics(df, column_name):\n",
    "    \"\"\"Calculate statistics for continuous/numeric columns\"\"\"\n",
    "    \n",
    "    stats = df.select(\n",
    "        F.avg(column_name).alias(\"average\"),\n",
    "        F.expr(f\"percentile_approx(`{column_name}`, 0.5)\").alias(\"median\"),\n",
    "        F.stddev(column_name).alias(\"standard_deviation\"),\n",
    "        F.min(column_name).alias(\"min\"),\n",
    "        F.max(column_name).alias(\"max\"),\n",
    "        F.expr(f\"percentile_approx(`{column_name}`, 0.25)\").alias(\"q1\"),\n",
    "        F.expr(f\"percentile_approx(`{column_name}`, 0.75)\").alias(\"q3\"),\n",
    "        F.variance(column_name).alias(\"variance\"),\n",
    "        F.skewness(column_name).alias(\"skewness\"),\n",
    "        F.kurtosis(column_name).alias(\"kurtosis\"),\n",
    "        F.count(F.when(F.col(column_name).isNull(), 1)).alias(\"missing_count\"),\n",
    "        F.count(\"*\").alias(\"total_count\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    # Calculate mode (most frequent value)\n",
    "    mode_df = df.groupBy(column_name).count().orderBy(F.desc(\"count\")).limit(1).collect()\n",
    "    mode_value = mode_df[0][column_name] if mode_df else None\n",
    "    \n",
    "    total_count = stats[\"total_count\"]\n",
    "    missing_count = stats[\"missing_count\"]\n",
    "    missing_percentage = (missing_count / total_count * 100) if total_count > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"average\": float(stats[\"average\"]) if stats[\"average\"] is not None else None,\n",
    "        \"median\": float(stats[\"median\"]) if stats[\"median\"] is not None else None,\n",
    "        \"mode\": float(mode_value) if mode_value is not None else None,\n",
    "        \"standard_deviation\": float(stats[\"standard_deviation\"]) if stats[\"standard_deviation\"] is not None else None,\n",
    "        \"min\": float(stats[\"min\"]) if stats[\"min\"] is not None else None,\n",
    "        \"max\": float(stats[\"max\"]) if stats[\"max\"] is not None else None,\n",
    "        \"q1\": float(stats[\"q1\"]) if stats[\"q1\"] is not None else None,\n",
    "        \"q3\": float(stats[\"q3\"]) if stats[\"q3\"] is not None else None,\n",
    "        \"skewness\": float(stats[\"skewness\"]) if stats[\"skewness\"] is not None else None,\n",
    "        \"variance\": float(stats[\"variance\"]) if stats[\"variance\"] is not None else None,\n",
    "        \"kurtosis\": float(stats[\"kurtosis\"]) if stats[\"kurtosis\"] is not None else None,\n",
    "        \"missing_count\": int(missing_count),\n",
    "        \"missing_percentage\": round(missing_percentage, 3)\n",
    "    }\n",
    "\n",
    "print(\"✓ Continuous statistics function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "042b9cc4-bd14-4c9b-870b-a32afc0ebedd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Helper Functions - Categorícal Statistics (Python)"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_categorical_statistics(df, column_name):\n",
    "    \"\"\"Calculate statistics for categorical columns\"\"\"\n",
    "    \n",
    "    total_count = df.count()\n",
    "    missing_count = df.filter(F.col(column_name).isNull()).count()\n",
    "    \n",
    "    # Get frequency distribution\n",
    "    freq_df = df.groupBy(column_name)\\\n",
    "        .count()\\\n",
    "        .filter(F.col(column_name).isNotNull())\\\n",
    "        .orderBy(F.desc(\"count\"))\\\n",
    "        .collect()\n",
    "    \n",
    "    distinct_categories = [row[column_name] for row in freq_df]\n",
    "    unique_category_count = len(distinct_categories)\n",
    "    \n",
    "    # Calculate frequency distribution with percentages\n",
    "    valid_count = total_count - missing_count\n",
    "    frequency_distribution = []\n",
    "    \n",
    "    for row in freq_df:\n",
    "        category = row[column_name]\n",
    "        count = row[\"count\"]\n",
    "        percentage = (count / valid_count * 100) if valid_count > 0 else 0\n",
    "        \n",
    "        frequency_distribution.append({\n",
    "            \"category\": str(category),\n",
    "            \"count\": int(count),\n",
    "            \"percentage\": round(percentage, 2)\n",
    "        })\n",
    "    \n",
    "    # Mode is the most frequent category\n",
    "    mode = distinct_categories[0] if distinct_categories else None\n",
    "    \n",
    "    missing_percentage = (missing_count / total_count * 100) if total_count > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"distinct_categories\": [str(c) for c in distinct_categories[:100]],  # Limit to 100\n",
    "        \"unique_category_count\": unique_category_count,\n",
    "        \"frequency_distribution\": frequency_distribution[:50],  # Limit to top 50\n",
    "        \"mode\": str(mode) if mode is not None else None,\n",
    "        \"missing_count\": int(missing_count),\n",
    "        \"missing_percentage\": round(missing_percentage, 3)\n",
    "    }\n",
    "\n",
    "print(\"✓ Categorical statistics function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "127c29ee-2313-4643-adc4-2b662650e287",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Main Processing Function (Python)"
    }
   },
   "outputs": [],
   "source": [
    "def process_parquet_file_grouped_by_family(file_path):\n",
    "    \"\"\"\n",
    "    Process a single parquet file and generate statistics for all columns grouped by the 'family' column.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    # Read parquet file\n",
    "    df = spark.read.parquet(file_path)\n",
    "    total_rows = df.count()\n",
    "    \n",
    "    print(f\"Total rows: {total_rows:,}\")\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "\n",
    "    # Check if 'family' column exists\n",
    "    if \"family\" not in df.columns:\n",
    "        raise KeyError(\"'family' column does not exist in the dataset!\")\n",
    "\n",
    "    # Get unique family classifications\n",
    "    family_groups = df.select(\"family\").distinct().collect()\n",
    "    total_families = len(family_groups)\n",
    "    print(f\"Unique families: {total_families}\")\n",
    "    \n",
    "    documents = []\n",
    "\n",
    "    # Loop through each family group\n",
    "    for idx, family_row in enumerate(family_groups, 1):\n",
    "        family_name = family_row[\"family\"]\n",
    "        print(f\"\\n[{idx}/{total_families}] Processing family: {family_name}\")\n",
    "\n",
    "        # Filter DataFrame for the current family\n",
    "        filtered_df = df.filter(F.col(\"family\") == family_name)\n",
    "        row_count = filtered_df.count()\n",
    "\n",
    "        # Track feature counts\n",
    "        features = {}\n",
    "        continuous_count = 0\n",
    "        categorical_count = 0\n",
    "        total_errors = 0\n",
    "\n",
    "        # Get columns to process (exclude the \"family\" classification column itself)\n",
    "        feature_columns = [col for col in df.columns if col != \"family\"]\n",
    "        total_features = len(feature_columns)\n",
    "\n",
    "        print(f\"  Processing {total_features} feature columns... \", end=\"\")\n",
    "\n",
    "        for col_name in feature_columns:\n",
    "            col_type = dict(df.dtypes)[col_name]\n",
    "\n",
    "            # If column is numeric, calculate continuous statistics\n",
    "            if is_numeric_column(col_type):\n",
    "                try:\n",
    "                    stats = calculate_continuous_statistics(filtered_df, col_name)\n",
    "                    features[col_name] = {\n",
    "                        \"data_type\": \"continuous\",\n",
    "                        \"statistics\": stats\n",
    "                    }\n",
    "                    continuous_count += 1\n",
    "                except Exception as e:\n",
    "                    total_errors += 1\n",
    "                    print(f\"\\n    Warning: Error calculating for {col_name}: {e}\")\n",
    "\n",
    "            # If column is categorical, calculate categorical statistics\n",
    "            elif is_categorical_column(col_type):\n",
    "                try:\n",
    "                    stats = calculate_categorical_statistics(filtered_df, col_name)\n",
    "                    features[col_name] = {\n",
    "                        \"data_type\": \"categorical\",\n",
    "                        \"statistics\": stats\n",
    "                    }\n",
    "                    categorical_count += 1\n",
    "                except Exception as e:\n",
    "                    total_errors += 1\n",
    "                    print(f\"\\n    Warning: Error calculating for {col_name}: {e}\")\n",
    "\n",
    "        print(f\"Done!\")\n",
    "        print(f\"  ✓ Continuous: {continuous_count}, Categorical: {categorical_count}, Errors: {total_errors}\")\n",
    "\n",
    "        # Create the summary document for this family\n",
    "        document = {\n",
    "            \"id\": f\"FAMILY-STATS-{family_name}-{BATCH_ID}\",\n",
    "            \"pk\": family_name,\n",
    "            \"type\": \"feature_statistics\",\n",
    "            \"family\": family_name,\n",
    "            \"batch_info\": {\n",
    "                \"current_batch_id\": BATCH_ID,\n",
    "                \"processed_date\": datetime.utcnow().isoformat() + \"Z\",\n",
    "                \"total_rows_processed\": row_count,\n",
    "                \"total_columns_processed\": total_features,\n",
    "                \"source_file\": file_path.replace(\"dbfs:\", \"\")\n",
    "            },\n",
    "            \"features\": features,\n",
    "            \"summary\": {\n",
    "                \"total_features\": total_features,\n",
    "                \"continuous_features\": continuous_count,\n",
    "                \"categorical_features\": categorical_count,\n",
    "                \"total_records_in_family\": row_count,\n",
    "                \"total_errors\": total_errors\n",
    "            },\n",
    "            \"version\": \"1.0\",\n",
    "            \"schema_version\": \"1.0\",\n",
    "            \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "            \"updated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "            \"source_system\": \"databricks\"\n",
    "        }\n",
    "\n",
    "        documents.append(document)\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Completed processing: {file_path}\")\n",
    "    print(f\"  Total families processed: {total_families}\")\n",
    "    print(f\"  Total documents created: {len(documents)}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "print(\"✓ Optimized processing function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dba23d6-b4c6-452a-89c6-4f2b44762b06",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CosmosDB Insert Function (Python)"
    }
   },
   "outputs": [],
   "source": [
    "def insert_to_cosmosdb_spark(documents, batch_size=20):\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        docs_json = [json.dumps(doc, default=str) for doc in batch]\n",
    "        df = spark.read.json(spark.sparkContext.parallelize(docs_json))\n",
    "\n",
    "        (\n",
    "            df.write.format(\"cosmos.oltp\")\n",
    "            .option(\"spark.cosmos.accountEndpoint\", COSMOS_ENDPOINT)\n",
    "            .option(\"spark.cosmos.accountKey\", COSMOS_KEY)\n",
    "            .option(\"spark.cosmos.database\", DB_NAME)\n",
    "            .option(\"spark.cosmos.container\", CONTAINER_NAME)\n",
    "            .option(\"spark.cosmos.write.strategy\", \"ItemOverwrite\")\n",
    "            .option(\"spark.cosmos.write.bulk.enabled\", \"true\")\n",
    "            .option(\"spark.cosmos.write.maxRetryCount\", \"3\")\n",
    "            .mode(\"append\")\n",
    "            .save()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "122c8e18-66cb-404c-814d-6101a62ebbfb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Process Parquet Files"
    }
   },
   "outputs": [],
   "source": [
    "# Process all parquet files\n",
    "all_documents = []\n",
    "\n",
    "for family_name, file_path in FILE_PATHS.items():\n",
    "    try:\n",
    "        documents = process_parquet_file_grouped_by_family(file_path)\n",
    "        all_documents.extend(documents)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {family_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Total documents generated: {len(all_documents)}\")\n",
    "print(f\"{'='*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f40888d3-bff4-4f24-9055-a0f5a0ce673c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Execute Processing Pipeline (Python)"
    }
   },
   "outputs": [],
   "source": [
    "# CELL 11: Execute Insert - MODIFIED (change function name)\n",
    "if all_documents:\n",
    "    # Reduce batch size and increase partitions to avoid memory issues\n",
    "    def insert_to_cosmosdb_spark(documents, batch_size=50):\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch = documents[i:i+batch_size]\n",
    "            docs_json = [json.dumps(doc, default=str) for doc in batch]\n",
    "            df = spark.read.json(spark.sparkContext.parallelize(docs_json))\n",
    "            df = df.repartition(12)  # Increase partition count if needed\n",
    "\n",
    "            df.write.format(\"cosmos.oltp\") \\\n",
    "                .option(\"spark.cosmos.accountEndpoint\", COSMOS_ENDPOINT) \\\n",
    "                .option(\"spark.cosmos.accountKey\", COSMOS_KEY) \\\n",
    "                .option(\"spark.cosmos.database\", DB_NAME) \\\n",
    "                .option(\"spark.cosmos.container\", CONTAINER_NAME) \\\n",
    "                .option(\"spark.cosmos.write.strategy\", \"ItemOverwrite\") \\\n",
    "                .option(\"spark.cosmos.write.bulk.enabled\", \"true\") \\\n",
    "                .option(\"spark.cosmos.write.maxRetryCount\", \"3\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .save()\n",
    "    insert_to_cosmosdb_spark(all_documents)\n",
    "else:\n",
    "    print(\"⚠ No documents to insert!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Test-21 Feature Statistics Generator for CosmosDB",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
