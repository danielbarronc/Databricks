{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d308513b-75ca-4baa-b70d-09b043649c7f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Parquet File Solution"
    }
   },
   "outputs": [],
   "source": [
    "# File path for the Excel file and DBFS storage\n",
    "source_blob_path = \"abfss://blobstorage@datablobstorage001.dfs.core.windows.net/landingzone/Excel\"\n",
    "local_file_path = \"/dbfs/dbfs/tmp/current_excel_file.xlsx\"  # Local temp file in DBFS\n",
    "local_file_path_1 = \"dbfs:/dbfs/tmp/current_excel_file.xlsx\"\n",
    "parquet_folder_path = \"/dbfs/tmp/parquet_files\"  # Parquet file storage location on DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f2eabbd-ffd5-41f7-8d3e-34105a759a7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "files = dbutils.fs.ls(source_blob_path)\n",
    "excel_files = [f.path for f in files if f.path.endswith(\".xlsx\")]\n",
    "\n",
    "if not excel_files:\n",
    "    raise FileNotFoundError(f\"No .xlsx files found in {blob_path}\")\n",
    "    \n",
    "# Use the first Excel file detected\n",
    "excel_file_remote = excel_files[0]\n",
    "dbutils.fs.cp(excel_file_remote, local_file_path)\n",
    "print(f\"Copied file {excel_file_remote} to {local_file_path}\")\n",
    "files1 = dbutils.fs.ls(\"/dbfs/tmp/current_excel_file.xlsx\")\n",
    "print(\"File Excel in DBFS\", files1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64d5efd8-e70e-4227-96c8-cbc0e3626663",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from openpyxl import load_workbook\n",
    "import os\n",
    "\n",
    "# Copy from DBFS to local file system\n",
    "dbfs_path = local_file_path\n",
    "local_path = \"/tmp/current_excel_file.xlsx\"\n",
    "\n",
    "if not os.path.exists(dbfs_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"File not found at {dbfs_path}. Please upload the file to DBFS using the Databricks UI or dbutils.fs.cp.\"\n",
    "    )\n",
    "    \n",
    "shutil.copy(dbfs_path, local_path)\n",
    "\n",
    "try:\n",
    "    workbook = load_workbook(local_path, read_only=True)\n",
    "    sheet_names = workbook.sheetnames\n",
    "    print(sheet_names)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to fetch sheet names: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e844bb2-d4b8-460c-921b-2c5bf6abc01c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.mkdirs(\"/dbfs/dbfs/parquet_files\")\n",
    "# dbutils.fs.rm(\"dbfs:/dbfs/tmp/parquet_files\", recurse=True)\n",
    "# dbutils.fs.rm(\"dbfs:/dbfs/dbfs/parquet_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce46a8b5-7753-4187-b1a6-33f62db4a984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# List of sheet names from the Excel file\n",
    "print(sheet_names)\n",
    "\n",
    "# Function to clean sheet names\n",
    "def clean_sheet_name(sheet_name):\n",
    "    \"\"\"\n",
    "    Cleans a sheet name to make it suitable for use as a parquet file name.\n",
    "    Replaces invalid characters with underscores, removes trailing/leading spaces, \n",
    "    and ensures lowercase for case-insensitivity.\n",
    "    \"\"\"\n",
    "    sanitized_name = re.sub(r\"[^\\w]\", \"_\", sheet_name.strip())  # Replace non-alphanumeric characters with \"_\"\n",
    "    sanitized_name = re.sub(r\"__+\", \"_\", sanitized_name)  # Replace multiple underscores with a single \"_\"\n",
    "    return sanitized_name.lower()  # Convert to lowercase\n",
    "\n",
    "def does_parquet_file_exist(sheet_name, base_dir):\n",
    "    \"\"\"\n",
    "    Checks if a Parquet file for the sheet exists in the specified directory.\n",
    "    :param sheet_name: Name of the sheet.\n",
    "    :param base_dir: Directory where Parquet files are stored.\n",
    "    :return: True if the Parquet file exists, False otherwise.\n",
    "    \"\"\"\n",
    "    parquet_file_path = f\"{base_dir}/{sheet_name}.parquet\"\n",
    "    \n",
    "    try:\n",
    "        # Attempt to list the file's parent directory\n",
    "        parent_dir = parquet_file_path.rsplit(\"/\", 1)[0]\n",
    "        files = dbutils.fs.ls(parent_dir)\n",
    "        print(\"files: \", files)\n",
    "        # Check if the exact file exists\n",
    "        return any(file.path == parquet_file_path or file.path.rstrip(\"/\") == parquet_file_path for file in files)\n",
    "    except Exception:\n",
    "        # Directory or file doesn't exist\n",
    "        return False\n",
    "\n",
    "cleaned_sheet_names = [clean_sheet_name(sheet_name) for sheet_name in sheet_names]\n",
    "# Print the original and cleaned sheet names\n",
    "print(\"Original and Cleaned Sheet Names:\")\n",
    "for original, cleaned in zip(sheet_names, cleaned_sheet_names):\n",
    "    print(f\"Original: {original} --> Cleaned: {cleaned}\")\n",
    "\n",
    "# Verify if the Parquet file exists for each sheet\n",
    "parquet_file_path = \"/dbfs/dbfs/parquet_files\"\n",
    "dir_parquet_files = {}\n",
    "for sheet_name in cleaned_sheet_names:\n",
    "    if does_parquet_file_exist(sheet_name, parquet_file_path):\n",
    "        print(f\"Parquet file for sheet '{sheet_name}' already exists.\")\n",
    "        dir_parquet_files[sheet_namer] = True\n",
    "    else:\n",
    "        print(f\"Parquet file for sheet '{sheet_name}' does not exist.\")\n",
    "        dir_parquet_files[sheet_name] = False\n",
    "\n",
    "print(dir_parquet_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08944f3b-fd79-44f2-bfa3-5fef00ebcce8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from pyspark.sql.functions import when, lit, col, regexp_replace\n",
    "from pyspark.sql.types import DecimalType, LongType\n",
    "\n",
    "\n",
    "# Step 1: Fully clean and map original column names to cleaned names\n",
    "def map_and_clean_column_names(columns):\n",
    "    \"\"\"\n",
    "    Cleans and maps column names:\n",
    "    - Strips leading/trailing whitespace and newlines.\n",
    "    - Replaces invalid characters (e.g., spaces, symbols) with underscores.\n",
    "    - Converts names to lowercase.\n",
    "    Returns:\n",
    "    - A list of cleaned column names.\n",
    "    - A mapping dictionary (original -> cleaned).\n",
    "    \"\"\"\n",
    "    original_to_cleaned = {\n",
    "        col_name.strip(): re.sub(r\"[^\\w]\", \"_\", col_name.strip()).lower()\n",
    "        for col_name in columns\n",
    "    }\n",
    "    cleaned_columns = list(original_to_cleaned.values())\n",
    "    return cleaned_columns, original_to_cleaned\n",
    "\n",
    "\n",
    "# Step 2: Dynamically align and reconcile schema\n",
    "def align_schema_dynamically(df, parquet_path, original_to_cleaned):\n",
    "    \"\"\"\n",
    "    Aligns the schema of the DataFrame dynamically:\n",
    "    - Adds missing columns as nulls.\n",
    "    - Aligns columns with the Parquet schema (if it exists).\n",
    "    - Ensures column order consistency.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load existing Parquet schema if the file exists\n",
    "        if os.path.exists(parquet_path):\n",
    "            existing_parquet_df = spark.read.parquet(parquet_path)\n",
    "            existing_schema = {\n",
    "                col_name.strip(): dtype for col_name, dtype in existing_parquet_df.dtypes\n",
    "            }\n",
    "            print(f\"[Info] Successfully loaded existing schema from {parquet_path}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Parquet file not found: {parquet_path}\")\n",
    "    except Exception as e:\n",
    "        # Fallback to the current DataFrame schema if no Parquet file exists\n",
    "        print(f\"[Warning] No Parquet schema found: {e}. Using DataFrame schema as baseline.\")\n",
    "        existing_schema = {col_name.strip(): dtype for col_name, dtype in df.dtypes}\n",
    "\n",
    "    # Reverse mapping: Cleaned to Original Names\n",
    "    cleaned_to_original = {v: k for k, v in original_to_cleaned.items()}\n",
    "\n",
    "    # Add missing columns dynamically\n",
    "    for col_name, col_dtype in existing_schema.items():\n",
    "        cleaned_col_name = original_to_cleaned.get(col_name.strip(), col_name)  # Handle cleaned names\n",
    "        if cleaned_col_name not in df.columns:\n",
    "            print(f\"[Adding Missing Column] Original: {col_name}, Cleaned: {cleaned_col_name}\")\n",
    "            df = df.withColumn(cleaned_col_name, lit(None).cast(col_dtype))  # Add missing column\n",
    "\n",
    "    # Reorder DataFrame columns to match Parquet schema\n",
    "    ordered_columns = [original_to_cleaned.get(col, col) for col in existing_schema.keys()]\n",
    "    df = df.select(*ordered_columns)\n",
    "    print(f\"[Schema Alignment Debug] Aligned Columns: {ordered_columns}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Step 3: Normalize numeric columns\n",
    "def normalize_numeric_columns(df):\n",
    "    \"\"\"\n",
    "    Normalizes numeric columns stored as strings by applying these rules:\n",
    "    - If the column contains letters, leave it as StringType.\n",
    "    - If the column is numeric with commas or decimals, convert it to DecimalType.\n",
    "    - If the column is numeric without commas or decimals, convert it to LongType.\n",
    "    \"\"\"\n",
    "    for col_name, dtype in df.dtypes:\n",
    "        # Skip non-string columns, as there's no normalization required for them\n",
    "        if dtype != \"string\":\n",
    "            continue\n",
    "\n",
    "        print(f\"[Normalizing Numeric Column] {col_name}\")\n",
    "\n",
    "        # Clean and remove invalid characters like spaces and commas for processing\n",
    "        cleaned_col = regexp_replace(col(col_name), \"[^\\d.,a-zA-Z]\", \"\")\n",
    "\n",
    "        # Apply rules:\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            when(cleaned_col.rlike(r\"[a-zA-Z]+\"), col(col_name))  # Leave as-is if it contains letters\n",
    "            .when(cleaned_col.rlike(r\"^\\d+,\\d+$|^\\d+\\.\\d+$\"), cleaned_col.cast(DecimalType(38, 10)))  # Decimal case\n",
    "            .when(cleaned_col.rlike(r\"^\\d+$\"), cleaned_col.cast(LongType()))  # Integer case\n",
    "            .otherwise(col(col_name))  # Retain original content if none of the rules match\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "# Step 4: Process all sheets from the Excel file\n",
    "def process_excel_sheets(sheet_names, excel_file_path, parquet_dir):\n",
    "    \"\"\"\n",
    "    Processes all sheets in an Excel file:\n",
    "    - Cleans and normalizes schema.\n",
    "    - Dynamically aligns and writes to Parquet.\n",
    "    \"\"\"\n",
    "    for sheet_name in sheet_names:\n",
    "        try:\n",
    "            print(f\"\\n[Processing Sheet] {sheet_name}\")\n",
    "\n",
    "            # Load sheet as DataFrame\n",
    "            df = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferSchema\", \"true\") \\\n",
    "                .option(\"dataAddress\", f\"'{sheet_name}'!A1\") \\\n",
    "                .load(excel_file_path)\n",
    "\n",
    "            print(f\"[Original Columns Debug] {sheet_name}: {df.columns}\")\n",
    "\n",
    "            # Step 1: Clean column names and create mapping\n",
    "            cleaned_columns, original_to_cleaned = map_and_clean_column_names(df.columns)\n",
    "\n",
    "            # Rename all columns in DataFrame according to the mapping\n",
    "            for original_col, cleaned_col in original_to_cleaned.items():\n",
    "                if original_col != cleaned_col:\n",
    "                    df = df.withColumnRenamed(original_col, cleaned_col)\n",
    "\n",
    "            print(f\"[Cleaned Columns Debug] {sheet_name}: {df.columns}\")\n",
    "\n",
    "            # Step 2: Define Parquet path\n",
    "            sanitized_sheet_name = re.sub(r\"[^\\w]\", \"_\", sheet_name.strip().lower())\n",
    "            parquet_path = os.path.join(parquet_dir, f\"{sanitized_sheet_name}.parquet\")\n",
    "\n",
    "            # Step 3: Align schema dynamically\n",
    "            df = align_schema_dynamically(df, parquet_path, original_to_cleaned)\n",
    "\n",
    "            # Step 4: Normalize numeric columns\n",
    "            df = normalize_numeric_columns(df)\n",
    "\n",
    "            # Step 5: Debug schema before writing\n",
    "            print(f\"[Final Schema Debug] {sheet_name}: {df.schema.simpleString()}\")\n",
    "\n",
    "            # Step 6: Write aligned and cleaned DataFrame to Parquet\n",
    "            df.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "            print(f\"[Success] Sheet '{sheet_name}' written to Parquet at: {parquet_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Failed to process sheet: {sheet_name}. Error: {e}\")\n",
    "\n",
    "\n",
    "# Example Execution\n",
    "local_excel_path = \"/dbfs/tmp/current_excel_file.xlsx\"\n",
    "parquet_dir = \"/dbfs/tmp/parquet_files\"\n",
    "\n",
    "process_excel_sheets(sheet_names, local_excel_path, parquet_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c20e27e-ff6d-47f3-b54b-491e5cc2b75d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "CREATE OR REPLACE TEMPORARY VIEW\n",
    "bushing_parquet\n",
    "USING parquet\n",
    "OPTIONS (path \"/dbfs/tmp/parquet_files/bushings.parquet\");\n",
    "\n",
    "SELECT *\n",
    "FROM bushing_parquet\n",
    "where part_number IS NOT NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff2b98b-1949-4941-8ee4-97f7c5d3a18c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"mcmaster_url\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1761173055047}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"/dbfs/tmp/parquet_files/bushings.parquet\"\n",
    "df = spark.read.parquet(file_path)\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6819755009226086,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Test-20-Troubleshooting-Excel-Corrupted-File",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
